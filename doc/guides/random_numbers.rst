.. _random_numbers:

Randomness in NEST Simulations
==============================

Introduction
------------

Random numbers are used for a variety of purposes in neuronal network
simulations, e.g.,

1. during node creation to randomize node parameters;

#. during connection creation 

   a. to select connections to be created,
   #. to parameterize connections;

#. during simulation 

   a. to generate stochastic input (randomized spike trains, noisy currents),
   #. to support stochastic spike generation.

This document discusses how NEST provides random numbers for these
purposes, how you can choose which random number generator (RNG) to
use, and how to set the seed of RNGs in NEST. We use the term "random
number" here for ease of writing, even though we are always talking
about pseudorandom numbers generated by some algorithm.


.. admonition:: NEST 3 vs NEST 2

   This document describes random number generators in NEST 3, which differs 
   significantly from earlier versions. If you are working with NEST 2, please
   consult the documentation for NEST 2.

   Due to the significant changes in random number architecture between NEST 2
   and NEST 3, it is not possible to re-create NEST 2 simulations with identical
   random number sequences in NEST 3. Thus, spike patterns will differ in detail
   between simulations of the same model with NEST 2 and NEST 3, but shall agree
   as long as suitable quantities, such as firing rates, correlations or spectra
   are compared across multiple simulations.


.. admonition:: Compiler dependency

   NEST 3 uses random generators provided by the C++ Standard Library to 
   obtain random deviates, i.e., random numbers following different distributions
   such as normal, binomial or Poissonian. Different versions of the C++
   Standard Library use different algorithms to generate these deviates.
   Therefore, simulation results can differ in detail depending on the compiler
   and this the C++ Standard Library implementation used. Note in particular that
   GCC and Clang come with different library implementations. 
   
   As for the difference between NEST 2 and NEST 3, simulations based on
   different compilers/standard libraries shall agree as long as suitable
   quantities are compared using suitable statistical tests. 
   

.. _working_with_rngs:

Working with randomness in NEST
-------------------------------

NEST 3 makes randomness easy: Setting a single seed gives you full
control over randomness throughout a simulation, and you can choose seed values
at liberty. Using NEST's powerful mechanism to :ref:`parameterize nodes and synapses <REFERENCE NEEDED>`,
and NEST's :ref:`probabilistic connection rules <connection_managment>` allow you 
to construct your network with parameters with a wide range of random properties,
from uniformly distributed numbers drawn from a fixed interval to mathematical
expressions combining spatial information with probability distributions. Neuron, device
and synapse models apply randomness during simulation as described in the model documentation.

This section provides a practical introduction into managing random number 
generators and seeds in NEST, followed by an :ref:`overview of the nest.random module <nest_random>`
and :ref:`examples of using randomness <random_examples>`.  A separate section provides :ref:`suggestions for additional
randomization from the Python level <python_rand>`. 

Programs which are entirely serial typically use a single source of random numbers.
In parallel simulations, such a single source of random numbers would form an
unacceptable bottleneck. To avoid this, NEST uses parallel streams of random numbers.
See section :ref:`Random number internals <random_internals>` if you are interested
in the technical details.


Selecting the type of random number generator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NEST provides a different random number generators, presently taken from the 
C++ Standard Library and the `Random123 <https://www.deshawresearch.com/resources_random123.html>`__
library. To see all available generators, run

::

    nest.GetKernelStatus('rng_types')
    
This currently gives the following generators:

::

     'Philox_32', 'Philox_64',
     'Threefry_32', 'Threefry_64',
     'mt19937', 'mt19937_64'

The Philox and Threefry generators are cryptographic generators from Random123, while mt19937
is the `Mersenne Twister generator <<http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html>`__.
Each generator comes in a 32- and a 64-bit implementation. The number of bits has no consequence
for the quality of randomness, but depending on computer and compiler, one or the other may show
better performance. More generators may be added in the future. Currently, only a small number of 
generators is available because we require generators with sufficiently long period as discussed in
:ref:`Random number internals <random_internals>`.

The default random number generator set in NEST 3 is `mt19937_64`. To choose a different generator,
simply set it

::

     nest.SetKernelStatus({'rng_type': 'Philox_32'})
     
It is a good idea to cross-check your simulation results using a different random number generator
type. Even though generators and our understanding of them has become much better in recent years,
there always remains a risk of RNG artifacts affecting simulations.


Seeding the random number generator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NEST uses a built-in default seed if you do not specify one. This means that unless you explicitly
set the seed, all simulations will be run with the same sequence of random numbers. So *do not forget
too seed your simulations*!

You can use any number :math:`s` with :math:`1\leq s \leq 2^{31}-1` as seed:

::

    nest.SetKernelStatus({'rng_seed': 12345})
    
As long as you use two different seed values, NEST will ensure that all random number streams in a
simulation are seeded properly; see :ref:`Random number internals <random_internals>` for details.

You can inspect the seed value used with

::

    nest.SetKernelStatus({'rng_seed': 12345})
    
Any simulation run with the same seed shall return identical results (provided the same 
compiler/C++ Standard Library was used).


.. _nest_random:

The NEST random module
----------------------

THIS SHOULD PROVIDE A COMPREHENSIVE OVERVIEW OF ALL THAT IS IN THE nest.random MODULE.


.. _random_examples:

Examples of using randomness
----------------------------


Randomizing the membrane potential
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

::

    nest.Create('iaf_psc_alpha', 10000, {'V_m': nest.random.normal(mean=-60.0, std=10.0)}) 


AND MORE EXAMPLES, see NEST 2 to 3 guide


.. _python_rand:

Randomization from the Python level
-----------------------------------

In rare occasions, the capabilities provided by NEST for parameterizing your network will
not cover your needs and require additional randomization at the Python level. This requires
some additional consideration when performing simulations in parallel, especially when using
MPI parallelization. You may want to consult the section on :ref:`Random number internals <random_internals>`
before continuing.

A key principle of parallel simulation in NEST is that a simulation performed with a fixed
number of virtual processes :math:`N_{\text{vp}} = M \times T` shall produce identical results
independent of between how many MPI processes $M$ and threads $T$ the virtual processes are 
divided. To observe this principle also when randomizing from the Python level, it is essential
to create one Python random number generator per virtual process and use the random number
generator for the virtual process to which a node belongs (for synapses: the VP of the target
node).

We consider first an example setting a random membrane potential (but note that this simple randomization
could and should be done directly in NEST!). We use the `modern random packagei introduced with NumPy 1.17 <https://numpy.org/doc/stable/reference/random/>`__.

.. code-block: ipython

    import numpy as np
    
    n_vp = 4
    py_seed = 987654
    
    nest.SetKernelStatus({'total_num_virtual_procs': n_vp})
    nrns = nest.Create('iaf_psc_alpha', 12)
    
    rngs = [np.random.default_rng(py_seed + n) for n in range(n_vp)]
    
    for n in nest.GetLocalNodeCollection(nrns):
        n.set({'V_m': rngs[n.get('vp')].uniform()})
        
After creating the neurons, we create `n_vp` random number generators. We then use
`nest.GetLocalNodeCollection()` to obtain all those neurons that belong to the local 
MPI rank, since each rank can only access local nodes. We then use `n.get('vp')` to obtain
the virtual processes responsible for the node and use it to pick out the correct RNG, 
from which we draw the random membrane potential.

To parameterize a connection, we need to use the random generator for the virtual process of
the target neuron. Continuing from the example above, we can randomize the connection weight
as follows:

.. code-block: ipython

	nest.Connect(nrns, nrns, 'one_to_one')
	conns = nest.GetConnections()
	
	for c in conns:
	    c.set({'weight': rngs[nest.NodeCollection([s.get('target')]).get('vp')].uniform()})


.. admonition:
   TODO: The above is far too complicated and creates a lot of RNGs we don't need for many
   MPI processes. If we easily can map VPs to target threads, we could do
   
   rngs = {nest.vp_to_thread(vp): np.random.default_rng(py_seed + vp) for vp in range(n_vp)}
   
   for n in nest.GetLocalNodeCollection(nrns):
        n.set({'V_m': rngs[n.get('thread')].uniform()})

   for c in conns:
	    c.set({'weight': rngs[s.get('target')].uniform()})


.. _random_internals:

Random number internals
-----------------------

TO BE REVISED

For details of parallelization in NEST, please see `Parallel
Computing <parallel-computing.md>`__ and `Plesser et al
(2007) <http://dx.doi.org/10.1007/978-3-540-74466-5_71>`__. Here, we
just summarize a few basics.

-  NEST can parallelize simulations through *multi-threading*,
   *distribution* or a combination of the two.

-  A distributed simulation is spread across several processes under the
   control of MPI (Message Passing Interface). Each network node is
   *local* to exactly one process and complete information about the
   node is only available to that process. Information about each
   connection is stored by the process in which the connection target is
   local and is only available and changeable on that process.

-  Multi-threaded simulations run in a single process in a single
   computer. As a consequence, all nodes in a multi-threaded simulation
   are local.

-  Distribution and multi-threading can be combined by running identical
   numbers of threads in each process.

-  A serial simulation has a single process with a single seed.

-  From the NEST user perspective, distributed processes and threads are
   visible as **virtual processes**. A simulation distributed across
   \\(M\\) MPI processes with \\(T\\) threads each, has \\(N\_{vp} = M
   times T\\) virtual processes. It is a basic design principle of NEST
   that simulations shall generate *identical* results when run with a
   fixed \\(N\_{VP}\\), no matter how the virutal processes are broken
   down into MPI processes and threads.

-  Useful information can be obtained like this

   import nest nest.NumProcesses() # number of MPI processes nest.Rank()
   # rank of MPI process executing command
   nest.GetKernelStatus(['num\_processes']) # same as
   nest.NumProcesses() nest.GetKernelStatus(['local\_num\_threads']) #
   number of threads in present process (same for all processes)
   nest.GetKernelStatus(['total\_num\_virtual\_procs']) # N\_vp = M x T

-  When querying neurons, only very limited information is available for
   neurons on other MPI processes. Thus, before checking for specific
   information, you need to check if a node is local:

   n = nest.Create('iaf\_psc\_alpha') if nest.GetStatus(n, 'local')[0]:
   # GetStatus() returns list, pick element print nest.GetStatus(n,
   'vp') # virtual process "owning" node print nest.GetStatus(n,
   'thread') # thread in calling process "owning" node

Random numbers in parallel simulations
--------------------------------------

Ideally, all random numbers in a simulation should come from a single
RNG. This would require shipping truckloads of random numbers from a
central RNG process to all simulations processes and is thus
impractical, if not outright prohibitively costly. Therefore, parallel
simulation requires an RNG on each parallel process. Advances in RNG
technology give us today a range of RNGs that can be used in parallel,
with a quite high level of certainty that the resulting parallel streams
of random numbers are non-overlapping and uncorrelated. While the former
can be guaranteed, we are not aware of any generator for which the
latter can be proven.

How many generators in a simulation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In a typical PyNEST simulation running on \\(N\_{vp}\\) virtual
processes, we will encounter \\(2 N\_{vp} + 1\\) random number
generators:

| The global NEST RNG
| This generator is mainly used when creating connections using
  ``RandomDivergentConnect``.

| One RNG per VP in NEST
| These generators are used when creating connections using
  ``RandomConvergentConnect`` and to provide random numbers to nodes
  generating random output, e.g. the ``poisson_generator``.

| One RNG per VP in Python
| These generators are used to randomized node properties (e.g., the
  initial membrane potential) and connection properties (e.g., weights).

The generators on the Python level are not strictly necessary, as one
could in principle access the per-VP RNGs built into NEST. This would
require very tedious SLI-coding, though. We therefore recommend at
present that you use additional RNGs on the Python side.

Why a Global RNG in NEST
^^^^^^^^^^^^^^^^^^^^^^^^

In some situations, randomized decisions on different virtual processes
are not independent of each other. The most important case are
randomized divergent connections. The problem here is as follows. For
the sake of efficiency, NEST stores all connection information in the
virtual process (VP) to which the target of a connection resides (target
process). Thus, all connections are generated by this target process.
Now consider the task of generating 100 randomized divergent connections
emanating from a given source neuron while using 4 VPs. Then there
should be 25 targets on each VP *on average*, but actual numbers will
fluctuate. If independent processes on all VPs tried to choose target
neurons, we could never be sure that exactly 100 targets would be chosen
in total.

NEST thus creates divergent connections using a global RNG. This random
number generator provides the exact same sequence of random numbers on
each virtual process. Using this global RNG, each VP chooses 100 targets
from the entire network, but actually creates connections only for those
targets that reside on the VP. In practice, the global RNG is
implemented using one "clone" on each VP; NEST checks occasionally that
all these clones are synchronized, i.e., indeed generate identical
sequences.

Seeding the Random Generators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Each of the \\(N\_{vp}\\) random generators needs to be seeded with a
different seed to generate a different random number sequences. We
recommend that you choose a *master seed* ``msd`` and seed the
\\(2N\_{vp}+1\\) generators with seeds ``msd``, ``msd+1``, ...,
``msd+2*N_vp``. Master seeds for for independent experiments must differ
by at least \\(2N\_{vp}+1\\) . Otherwise, the same sequence(s) would
enter in several experiments.

Seeding the Python RNGs
^^^^^^^^^^^^^^^^^^^^^^^

You can create a properly seeded list of \\(N\_{vp}\\) RNGs on the
Python side using

::

    import numpy
    msd = 123456
    N_vp = nest.GetKernelStatus(['total_num_virtual_procs'])[0]
    pyrngs = [numpy.random.RandomState(s) for s in range(msd, msd+N_vp)]

``msd`` is the master seed, choose your own!

Seeding the global RNG
^^^^^^^^^^^^^^^^^^^^^^

The global NEST rng is seeded with a single, positive integer number:

::

    nest.SetKernelStatus({’grng_seed’ : msd+N_vp})

Seeding the per-process RNGs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The per-process RNGs are seeded by a list of \\(N\_{vp}\\) positive
integers:

::

    nest.SetKernelStatus({’rng_seeds’ : range(msd+N_vp+1, msd+2*N_vp+1)})

Choosing the random generator type
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Python and NumPy have the `MersenneTwister
MT19937ar <http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html>`__
random number generator built in. There is no simple way of choosing a
different generator in NumPy, but as the MT19937ar appears to be a very
robust generator, this should not cause significant problems.

NEST uses by default Knuth's lagged Fibonacci random number generator
(The Art of Computer Programming, vol 2, 3rd ed, 9th printing or later,
ch 3.6). If you want to use other generators, you can exchange them as
described below. If you have built NEST without the GNU Science Library
(GSL), you will only have the Mersenne Twister MT19937ar and Knuth's
lagged Fibonacci generator available. Otherwise, you will also have some
60 generators from the GSL at your disposal (not all of them
particularly good). You can see the full list of RNGs using

::

    nest.sli_run('rngdict info')

Setting a different global RNG
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To set a different global RNG in NEST, you have to pass a NEST random
number generator object to the NEST kernel. This can currently only be
done by writing some SLI code. The following code replaces the current
global RNG with MT19937 seeded with 101:

::

    nest.sli_run('<< /grng rngdict/MT19937 :: 101 CreateRNG >> SetKernelStatus')

The following happens here:

-  ``rngdict/MT19937 ::`` fetches a "factory" for MT19937 from the
   ``rngdict``

-  ``101 CreateRNG`` uses the factory to create a single MT19937
   generator with seed 101

-  This is generator is then passed to the ``/grng`` status variable of
   the kernel. This is a "write only" variable that is invisible in
   ``GetKernelStatus()``.

Setting different per-processes RNGs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

One always needs to exchange all \\(N\_{vp}\\) per-process RNGs at once.
This is done by (assuming \\(N\_{vp}=2\\) ):

::

    nest.sli_run('<< /rngs [102 103] { rngdict/MT19937 :: exch CreateRNG } Map >> SetKernelStatus')

The following happens here:

-  ``[102 103] { rngdict/MT19937 :: exch CreateRNG } Map`` creates an
   array of two RNG objects seeded with 102 and 103, respectively.

-  This array is then passed to the ``/rngs`` status variable of the
   kernel. This variable is invisible as well.


.. _examples-rng:

Examples
--------

**NOTE: These examples are not yet updated for NEST 2.4**

No random variables in script
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If no explicit random variables appear in your script, i.e., if
randomness only enters in your simulation through random stimulus
generators such as ``poisson_generator`` or randomized connection
routines such as ``RandomConvergentConnect``, you do not need to worry
about anything except choosing and setting your random seeds, possibly
exchanging the random number generators.

Randomizing the membrane potential
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want to randomize the membrane potential (or any other property
of a neuron), you need to take care that each node is updated by the
process on which it is local using the per-VP RNG for the VP to which
the node belongs. This is achieved by the following code

::

    pyrngs = [numpy.random.RandomState(s) for s in range(msd, msd+N_vp)]
    nodes   = nest.Create('iaf_psc_delta', 10)
    node_info   = nest.GetStatus(nodes)
    local_nodes = [(ni['global_id'], ni['vp']) for ni in node_info if ni['local']]
    for node_id,vp in local_nodes:
       nest.SetStatus([node_id], {'V_m': pyrngs[vp].uniform(-70.0, -50.0)})

The first line generates \\([N\_{vp}\\) properly seeded NumPy RNGs as
discussed above. The next line creates 10 nodes, while the third line
extracts status information about each node. For local nodes, this will
be full information, for non-local nodes we only get the following
fields: ``local``, ``model`` and ``type``. On the fourth line, we create
a list of tuples, containing global ID and virtual process number for
all local neurons. The for loop then sets the membrane potential of each
local neuron drawn from a uniform distribution on \\([-70, -50]\\) using
the Python-side RNG for the VP to which the neuron belongs.

Randomizing convergent connections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We continue the above example by creating random convergent connections,
\\(C\_E\\) connections per target node. In the process, we randomize the
connection weights:

::

    C_E = 10
    nest.CopyModel("static_synapse", "excitatory")
    for tgt_node_id, tgt_vp in local_nodes:
        weights = pyrngs[tgt_vp].uniform(0.5, 1.5, C_E)
        nest.RandomConvergentConnect(nodes, [tgt_node_id], C_E,
                                     weight=list(weights), delay=2.0,
                                     model="excitatory")

Here we loop over all local nodes considered as target nodes. For each
target, we create an array of \\(C\_E\\) randomly chosen weights,
uniform on \\([0.5, 1.5\\. We then call ``RandomConvergentConnect()``
with this weight list as argument. Note a few details:

-  We need to put ``tgt_node_id`` into brackets as PyNEST functions always
   expect lists of node IDs.

-  We need to convert the NumPy array ``weights`` to a plain Python
   list, as most PyNEST functions currently cannot handle array input.

-  If we specify ``weight``, we must also provide ``delay``.

You can check the weights selected by

::

    print nest.GetStatus(nest.GetConnections(), ['source', 'target', 'weight'])

which will print a list containing a triple of source node ID, target node ID
and weight for each connection in the network. If you want to see only a
subset of connections, pass source, target, or synapse model to
``GetConnections()``.

Randomizing divergent connections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Randomizing the weights (or delays or any other properties) of divergent
connections is more complicated than for convergent connections, because
the target for each connection is not known upon the call to
``RandomDivergentConnect``. We therefore need to first create all
connections (which we can do with a single call, passing lists of nodes
and targets), and then need to manipulate all connections. This is not
only more complicated, but also significantly slower than the example
above.

::

    nest.CopyModel('static_synapse', 'inhibitory', {'weight': 0.0, 'delay': 3.0})
    nest.RandomDivergentConnect(nodes, nodes, C_E, model='inhibitory')
    node_id_vp_map = dict(local_nodes)
    for src in nodes:
        conns = nest.GetConnections(source=[src], synapse_model='inhibitory')
        tgts = [conn[1] for conn in conns]
        rweights = [{'weight': pyrngs[node_id_vp_map[tgt]].uniform(-2.5, -0.5)}
                   for tgt in tgts]
        nest.SetStatus(conns, rweights)

In this code, we first create all connections with weight 0. We then
create ``node_id_vp_map``, mapping node IDs to VP number for all local nodes.
For each node considered as source, we then find all outgoing excitatory
connections from that node and then obtain a flat list of the targets of
these connections. For each target we then choose a random weight as
above, using the RNG pertaining to the VP of the target. Finally, we set
these weights. Note that the code above is **slow**. Future versions of
NEST will provide better solutions.

Testing scripts randomizing node or connection parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To ensure that you are consistently using the correct RNG for each node
or connection, you should run your simulation several times the same
\\(N\_{vp}\\), but using different numbers of MPI processes. To this
end, add towards the beginning of your script

::

    nest.SetKernelStatus({"total_num_virtual_procs": 4})

and ensure that spikes are logged to file in the current working
directory. Then run the simulation with different numbers of MPI
processes in separate directories

::

     mkdir 41 42 44
     cd 41
     mpirun -np 1 python3 test.py
     cd ../42
     mpirun -np 2 python3 test.py
     cd ../44
     mpirun -np 4 python3 test.py
     cd ..

These directories should now have identical content, something you can
check with ``diff``:

::

    diff 41 42
    diff 41 44

These commands should not generate any output. Obviously, this test
checks only a necessary, but by no means sufficient condition for a
correct simulation. (Oh yes, do make sure that these directories contain
data! Nothing easier than to pass a diff-test on empty dirs.)
